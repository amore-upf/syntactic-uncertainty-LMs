{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_model import dictionary_corpus\n",
    "from lstm_model import utils\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message='source.*has changed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def update_hidden(lm, lm_vocabulary, prompt, word, hidden, alpha = 0.5):\n",
    "    hidden_tmp = []\n",
    "    for i in range(len(hidden)):\n",
    "        hidden_tmp.append(Variable(hidden[i], requires_grad=True))\n",
    "    hidden_tmp = tuple(hidden_tmp)\n",
    "    output, hidden = lm(prompt, hidden_tmp)\n",
    "    output_scores = output.view(-1, len(lm_vocabulary.word2idx))[-1]\n",
    "    cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "    loss = cross_entropy(output_scores, torch.LongTensor(lm_vocabulary.word2idx[word]))\n",
    "    loss.backward()\n",
    "    hidden_new = []\n",
    "    for i in range(len(hidden)):\n",
    "        grad = hidden_tmp[i].grad.clone()\n",
    "        updated_hidden = hidden_tmp - alpha * grad\n",
    "        hidden_new.append(updated_hidden)\n",
    "    hidden_new = tuple(hidden_new)\n",
    "    return hidden_new\n",
    "\n",
    "\n",
    "# def generation_with_update(prompt, lm, lm_vocabulary, cuda = False, model_type ='lstm_gulordava', sentence = False, n_sentences = 1, max_len = 20,\n",
    "#               generation_type = 'greedy', filter_by = 'k', top_p = 0.5,top_k = 10, output_generations = 5, temperature = 1,\n",
    "#               repetition_penalty = 1):\n",
    "#     if sentence: text += '<eos>'\n",
    "#     if generation_type == 'greedy': output_generations = 1\n",
    "#     generated = []\n",
    "#     original_prompt = lm_vocabulary.encode(prompt)\n",
    "#     original_prompt = torch.LongTensor(original_prompt).cuda() if cuda else torch.LongTensor(original_prompt)\n",
    "#     original_prompt = utils.batchify(original_prompt, 1, cuda)\n",
    "#     while len(generated) < output_generations:\n",
    "#         tokens_generated = 0\n",
    "#         words_in_text = set([int(x) for x in original_prompt])\n",
    "#         end_of_sentence = False\n",
    "#         generated_text = ''\n",
    "#         n_sentences_generated = 0\n",
    "#         # TODO adapt to non-lstm\n",
    "#         hidden = lm.init_hidden(1)\n",
    "        \n",
    "#         output, hidden = lm(original_prompt[:-1], hidden)\n",
    "#         word = original_prompt[:-1]\n",
    "        \n",
    "#         hidden = update_hidden(lm, lm_vocabulary, prompt, word, hidden, alpha = 0.5)\n",
    "#         prompt = word\n",
    "        \n",
    "#         while n_sentences_generated != n_sentences or tokens_generated == max_len:\n",
    "#             output, hidden = lm(prompt, hidden)\n",
    "#             output_scores = output.view(-1, len(lm_vocabulary.word2idx))[-1]\n",
    "#             if temperature != 1:\n",
    "#                 output_scores /= (temperature)\n",
    "#             if repetition_penalty != 1:\n",
    "#                 for _ in words_in_text:\n",
    "#                     output_scores[_] /= (repetition_penalty)\n",
    "#             output_scores[encode_lstm('<unk>')[0]] /= - 10 ** 10 \n",
    "#             next_word_probability_distribution = F.log_softmax(output_scores, dim=-1)\n",
    "#             sorted_probabilities, sorted_words = torch.sort(next_word_probability_distribution, descending = True)\n",
    "#             if generation_type == 'greedy':\n",
    "#                 generated_word_id = greedy(sorted_words)\n",
    "#             elif generation_type == 'sampling':\n",
    "#                 generated_word_id = sampling(sorted_words, sorted_probabilities, filter_by= filter_by, top_k = top_k, top_p = top_p)\n",
    "#             generated_word_id = generated_word_id[0]\n",
    "#             generated_word = decode_lstm([generated_word_id])\n",
    "#             if generated_word == '<eos>':\n",
    "#                 n_sentences_generated += 1\n",
    "#             else:\n",
    "#                 generated_text += generated_word + ' '\n",
    "#                 words_in_text.add(generated_word_id)\n",
    "#             prompt = encode_lstm(generated_word)\n",
    "#             tokens_generated  += 1\n",
    "#         generated.append(generated_text)\n",
    "#     return generated\n",
    "\n",
    "lm, lm_vocabulary = load_lm(model_type = 'distilGPT2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(16791)\n",
      "tensor(16791)\n",
      "tensor(16791)\n",
      "tensor(16791)\n",
      "tensor(16791)\n",
      "tensor(16791)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n",
      "tensor(27)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-3af600a95daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m#x = lm_vocabulary.tokenizer.encode('My name is')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mgeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Every day is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_vocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'distilGPT2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# context_indices = lm_vocabulary.encode('Every day is')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-3af600a95daa>\u001b[0m in \u001b[0;36mgeneration\u001b[0;34m(original_prompt, lm, lm_vocabulary, cuda, model_type, sentence, n_sentences, max_len, generation_type, filter_by, top_p, top_k, output_generations, temperature, repetition_penalty)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn_sentences_generated\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_sentences\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokens_generated\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeploy_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_vocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lstm_gulordava'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0moutput_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-3af600a95daa>\u001b[0m in \u001b[0;36mdeploy_lm\u001b[0;34m(text, lm, lm_vocabulary, model_type, sentence, cuda, hidden)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlm_vocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0moutput_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlm_logits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "\n",
    "\n",
    "def greedy(sorted_words, n_samples=1):\n",
    "    return list(sorted_words[:n_samples])\n",
    "\n",
    "\n",
    "def sampling(sorted_words, sorted_probabilities, filter_by='k', top_k=10, top_p=0.3, n_samples=1):\n",
    "    stop_at = top_k\n",
    "    if filter_by == 'p':\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_probabilities, dim=-1), dim=-1)\n",
    "        stop_at = 0\n",
    "        while cumulative_probs[stop_at] < top_p:\n",
    "            stop_at += 1\n",
    "        stop_at += 1  # first token above the threshold\n",
    "    sampled_token = torch.multinomial(F.softmax(torch.Tensor(sorted_probabilities[:stop_at]), dim=-1),\n",
    "                                      num_samples=n_samples)\n",
    "    return [sorted_words[int(i)] for i in sampled_token]\n",
    "\n",
    "\n",
    "def deploy_lm(text, lm, lm_vocabulary,\n",
    "              model_type='lstm_gulordava', sentence=False, cuda=False, hidden = None):\n",
    "    if sentence: text += '<eos>'\n",
    "    text = lm_vocabulary.encode(text)\n",
    "    with torch.no_grad():\n",
    "        text = torch.LongTensor(text).cuda() if cuda else torch.LongTensor(text)   \n",
    "        if model_type == 'lstm_gulordava':\n",
    "            text = text.unsqueeze(1)\n",
    "            if hidden == None:\n",
    "                hidden = lm.init_hidden(1)\n",
    "            output, hidden = lm(text, hidden)\n",
    "            output_scores = output.view(-1, len(lm_vocabulary.word2idx))\n",
    "            return output_scores, hidden\n",
    "        elif lm_vocabulary.transformer:\n",
    "            text = text.unsqueeze(0)\n",
    "            outputs, _ = lm(text)\n",
    "            output_scores = outputs.squeeze(0)\n",
    "            return output_scores\n",
    "        \n",
    "#     next_word_scores = language_model(context)[0][-1]\n",
    "  \n",
    "#   # Trasform scores into probabilities through softmax function\n",
    "#   # probability distribution over the words in the vocabulary\n",
    "#   next_word_probability_distribution = F.softmax(next_word_scores, dim = -1)\n",
    "#   return next_word_probability_distribution\n",
    "        \n",
    "def generation(original_prompt, lm, lm_vocabulary, cuda=False, model_type='lstm_gulordava', sentence=False, n_sentences=1,\n",
    "               max_len=20,\n",
    "               generation_type='greedy', filter_by='k', top_p=0.5, top_k=10, output_generations=5, temperature=1,\n",
    "               repetition_penalty=1):\n",
    "    if sentence: text += '<eos>'\n",
    "    if generation_type == 'greedy': output_generations = 1\n",
    "    generated = []\n",
    "    while len(generated) < output_generations:\n",
    "        tokens_generated = 0\n",
    "        words_in_text = set([x for x in original_prompt])\n",
    "        end_of_sentence = False\n",
    "        generated_text = ''\n",
    "        n_sentences_generated = 0\n",
    "        hidden = lm.init_hidden(1) if model_type == 'lstm_gulordava' else None\n",
    "        prompt = original_prompt\n",
    "        while n_sentences_generated != n_sentences or tokens_generated == max_len:\n",
    "            output = deploy_lm(prompt, lm, lm_vocabulary, hidden = hidden, model_type =model_type)\n",
    "            if model_type == 'lstm_gulordava': output, hidden = output \n",
    "            output_scores = output[-1]\n",
    "            if temperature != 1:\n",
    "                output_scores /= (temperature)\n",
    "            if repetition_penalty != 1:\n",
    "                for _ in words_in_text:\n",
    "                    output_scores[_] /= (repetition_penalty)\n",
    "            output_scores[lm_vocabulary.encode('<unk>')[0]] /= - 10 ** 10\n",
    "            next_word_probability_distribution = F.log_softmax(output_scores, dim=-1)\n",
    "            sorted_probabilities, sorted_words = torch.sort(next_word_probability_distribution, descending=True)\n",
    "            print(sorted_probabilities[:10])\n",
    "            if generation_type == 'greedy':\n",
    "                generated_word_id = greedy(sorted_words)\n",
    "            elif generation_type == 'sampling':\n",
    "                generated_word_id = sampling(sorted_words, sorted_probabilities, filter_by=filter_by, top_k=top_k,\n",
    "                                             top_p=top_p)\n",
    "            generated_word_id = generated_word_id[0]\n",
    "            generated_word = lm_vocabulary.decode([generated_word_id])\n",
    "            if generated_word == '<eos>':\n",
    "                n_sentences_generated += 1\n",
    "            else:\n",
    "                generated_text += generated_word + ' '\n",
    "                words_in_text.add(generated_word_id)\n",
    "            if model_type == 'lstm_gulordava': \n",
    "                prompt = generated_word\n",
    "            else:\n",
    "                prompt += generated_word\n",
    "            tokens_generated += 1\n",
    "        generated.append(generated_text)\n",
    "    return generated\n",
    "\n",
    "\n",
    "#x = lm_vocabulary.tokenizer.encode('My name is')\n",
    "\n",
    "generation('Every day is', lm, lm_vocabulary, model_type = 'distilGPT2')\n",
    "\n",
    "# context_indices = lm_vocabulary.encode('Every day is')\n",
    "# print(context_indices)\n",
    "# context = torch.tensor(context_indices)\n",
    "# next_word_scores = language_model(context)[0][-1]\n",
    "# print(next_word_scores[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Everyone agrees that an old trash can smell'\n",
    "\n",
    "print(generation(text, lm, lm_vocabulary, cuda = cuda, generation_type = 'sampling', filter_by = 'p', top_k = 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOUNDARY_0 = 6\n",
    "BOUNDARY_1 = 7\n",
    "\n",
    "data = pd.read_csv('../data/RRCSimple Stimuli - RRC.csv', header = 0)\n",
    "#print(data)\n",
    "data.columns = ['id', 'sentence', 2,3 ]\n",
    "data = list(data.sentence)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].replace('.', ' . ')\n",
    "    prompt1 = ' '.join(data[i].split()[:BOUNDARY_0 + 1])\n",
    "    prompt2 = ' '.join(data[i].split()[:BOUNDARY_1 + 1])\n",
    "    print(prompt1, prompt2)\n",
    "    \n",
    "#     #print(beam_search(prompt1, lm, lm_vocabulary, model_type ='lstm_gulordava'))\n",
    "#     #print(generation(prompt1, lm, lm_vocabulary, cuda = cuda, generation_type = 'sampling', filter_by = 'p', top_k = 0.3))\n",
    "#     print(prompt2)\n",
    "#     #print(beam_search(prompt2, lm, lm_vocabulary, model_type ='lstm_gulordava'))\n",
    "#     print(generation(prompt2, lm, lm_vocabulary, cuda = cuda, generation_type = 'sampling', filter_by = 'p', top_k = 0.3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/SynApt PreExp2 Stimuli - Sheet1.csv', header = 0)\n",
    "data.columns = ['type', 0,'sentence', 2,3 ]\n",
    "data = data[data.type == 'NP/Z']\n",
    "data = list(data.sentence)\n",
    "\n",
    "BOUNDARY_0 = 5\n",
    "BOUNDARY_1 = 6\n",
    "\n",
    "for i in range(len(data[:10])):\n",
    "    data[i] = data[i].replace('.', ' . ')\n",
    "    prompt1 = ' '.join(data[i].split()[:BOUNDARY_0 + 1])\n",
    "    prompt2 = ' '.join(data[i].split()[:BOUNDARY_1 + 1])\n",
    "    print(prompt1, prompt2)\n",
    "    \n",
    "\n",
    "#     print(beam_search(prompt1, lm, lm_vocabulary, model_type ='lstm_gulordava'))\n",
    "#     #print(generation(prompt1, lm, lm_vocabulary, cuda = cuda, generation_type = 'sampling', filter_by = 'p', top_k = 0.3))\n",
    "#     print(prompt2)\n",
    "#     print(beam_search(prompt2, lm, lm_vocabulary, model_type ='lstm_gulordava'))\n",
    "#     print(generation(prompt2, lm, lm_vocabulary, cuda = cuda, generation_type = 'sampling', filter_by = 'p', top_k = 0.3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
