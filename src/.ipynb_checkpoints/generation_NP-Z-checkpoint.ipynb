{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_utils import *\n",
    "from generation_utils import *\n",
    "import pandas as  pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, lm_vocabulary = load_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrc_data = pd.read_csv('../data/sentences_RRC.csv')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrc_sentences = list(rrc_data.RRC)\n",
    "rrc_disambiguating = list(rrc_data.disambiguating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even RB advmod\n",
      "though IN mark\n",
      "the DT det\n",
      "girl NN nsubj\n",
      "phoned VBD advcl\n",
      "the DT det\n",
      "instructor NN nsubj\n",
      "was VBD ROOT\n",
      "very RB advmod\n",
      "upset JJ acomp\n",
      "with IN prep\n",
      "her PRP pobj\n",
      "for IN prep\n",
      "missing VBG pcomp\n",
      "a DT det\n",
      "lesson NN dobj\n",
      ". . punct\n"
     ]
    }
   ],
   "source": [
    "sent = \"Even though the girl phoned the instructor was very upset with her for missing a lesson. \"\n",
    "doc = nlp(sent)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.tag_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The experienced waitress cooked the grilled chicken .', 'The experienced waitress cooked the grilled chicken sandwich at a breakfast set .', 'The experienced waitress cooked the grilled chicken as well .', 'The experienced waitress cooked the grilled chicken cake into her heart .', 'The experienced waitress cooked the grilled chicken sandwich to keep it safe overnight .', 'The experienced waitress cooked the grilled chicken dinner together .', 'The experienced waitress cooked the grilled chicken as soon as she had cooked .'] ['The experienced waitress cooked the grilled chicken sent to it every day for a week .', 'The experienced waitress cooked the grilled chicken sent to her head .', 'The experienced waitress cooked the grilled chicken sent to hospital .', 'The experienced waitress cooked the grilled chicken sent .', 'The experienced waitress cooked the grilled chicken sent on .', 'The experienced waitress cooked the grilled chicken sent during the ceremony .', 'The experienced waitress cooked the grilled chicken sent by a local fisherman to rescue him .', 'The experienced waitress cooked the grilled chicken sent on a pizza .', 'The experienced waitress cooked the grilled chicken sent on sandwiches .', 'The experienced waitress cooked the grilled chicken sent to him for the rest of the voyage .']\n",
      "['The ailing mother prepared a hot meal for the needy .', 'The ailing mother prepared a hot meal .', 'The ailing mother prepared a hot meal for a woman so that she could return .', 'The ailing mother prepared a hot meal for lunch .', 'The ailing mother prepared a hot meal every day .', 'The ailing mother prepared a hot meal for him by <unk> .', 'The ailing mother prepared a hot meal which she asked .', 'The ailing mother prepared a hot meal which he had worked with ) .'] ['The ailing mother prepared a hot meal ate each morning .', 'The ailing mother prepared a hot meal ate pork .', 'The ailing mother prepared a hot meal ate .', 'The ailing mother prepared a hot meal ate with him .', 'The ailing mother prepared a hot meal ate on her floor .', 'The ailing mother prepared a hot meal ate \" en masse ...', 'The ailing mother prepared a hot meal ate every day .', 'The ailing mother prepared a hot meal ate dinner at lunch .', 'The ailing mother prepared a hot meal ate each night .']\n",
      "['The sleepy volunteers offered the hot soup .', 'The sleepy volunteers offered the hot soup food for soldiers and locals so that they could be used , along', 'The sleepy volunteers offered the hot soup as well .', 'The sleepy volunteers offered the hot soup with hot bread , and the meat of butter .', 'The sleepy volunteers offered the hot soup grown to the cook .', 'The sleepy volunteers offered the hot soup to eat vegetables for themselves .', \"The sleepy volunteers offered the hot soup to the chef 's assistant .\", 'The sleepy volunteers offered the hot soup of hot , hot meat .'] ['The sleepy volunteers offered the hot soup were served as a meal between Thanksgiving and June .', 'The sleepy volunteers offered the hot soup were eaten by men ; they came to be aware of their role', 'The sleepy volunteers offered the hot soup were soaked together .', 'The sleepy volunteers offered the hot soup were available , and they started going to lunch .', 'The sleepy volunteers offered the hot soup were ready .', 'The sleepy volunteers offered the hot soup were sold .', 'The sleepy volunteers offered the hot soup were prepared to pour food .', 'The sleepy volunteers offered the hot soup were available to customers .', 'The sleepy volunteers offered the hot soup were offered ; the food rations would be unloaded to help the Red', 'The sleepy volunteers offered the hot soup were allowed to join .']\n"
     ]
    }
   ],
   "source": [
    "def get_sentences(sent_portion, lm, lm_vocabulary):\n",
    "    sent_portion = lm_vocabulary.encode(sent_portion)\n",
    "    sent_portion = torch.LongTensor(sent_portion).unsqueeze(0)\n",
    "    generated = generate(sent_portion, lm, lm_vocabulary, do_sample = True, \n",
    "                   repetition_penalty = 2, num_return_sequences =  10, temperature = 2,\n",
    "                unknown_penalty = 1000000, top_k = 50, num_beams = 5, max_length = 20)  # generate sequence\n",
    "    sent_generated = []\n",
    "    for i in range(len(generated)):\n",
    "        s = lm_vocabulary.decode(generated[i])\n",
    "        s = list(nlp(s).sents)[0].text\n",
    "        if s not in sent_generated:\n",
    "            sent_generated.append(s)\n",
    "    sent_generated = sent_generated[:11]\n",
    "    return sent_generated\n",
    "\n",
    "sentences_per_item = []\n",
    "for sent, split in zip(rrc_sentences, rrc_disambiguating):\n",
    "    sent = sent.replace('.', ' .')\n",
    "    sent = sent.split()\n",
    "    sent_portion_1 = ' '.join(sent[:sent.index(split)])\n",
    "    sent_portion_2 = ' '.join(sent[:sent.index(split)+1])\n",
    "    if not lm_vocabulary.encode('<unk>')[0] in lm_vocabulary.encode(sent_portion_2):\n",
    "        sent1 = get_sentences(sent_portion_1, lm, lm_vocabulary)\n",
    "        sent2 = get_sentences(sent_portion_2, lm, lm_vocabulary)\n",
    "        print(sent1, sent2)\n",
    "        sentences_per_item.append((sent1, sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det volunteers NOUN []\n",
      "sleepy amod volunteers NOUN []\n",
      "volunteers nsubj were AUX [The, sleepy, given]\n",
      "given acl volunteers NOUN [soup]\n",
      "the det soup NOUN []\n",
      "hot amod soup NOUN []\n",
      "soup dobj given VERB [the, hot]\n",
      "were auxpass prepared ADJ [volunteers]\n",
      "prepared ROOT prepared ADJ [were, pour, .]\n",
      "to aux pour VERB []\n",
      "pour xcomp prepared ADJ [to, food]\n",
      "food dobj pour VERB []\n",
      ". punct prepared ADJ []\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The sleepy volunteers given the hot soup were prepared to pour food .\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "pkl.dump(sentences_per_item, open('../data/generated.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
