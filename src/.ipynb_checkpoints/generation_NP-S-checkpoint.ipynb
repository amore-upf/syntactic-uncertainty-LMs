{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_utils import *\n",
    "from generation_utils import *\n",
    "import pandas as  pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, lm_vocabulary = load_lm(model_type = 'distilGPT2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rrc_data = pd.read_csv('../data/sentences_RRC.csv')[:5]\n",
    "# rrc_sentences = list(rrc_data.RRC)\n",
    "# rrc_disambiguating = list(rrc_data.disambiguating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Even though the band left the party went on for at least another two hours.', \n",
    "            'As the couple danced the tango began to be played by a live orchestra.']\n",
    "\n",
    "sentences_long = ['Even though the band which played funk music left the party went on for at least another two hours.']\n",
    "\n",
    "sentences_unambig = ['Even though the band left , the party went on for at least another two hours . ', \n",
    "                    ' As the couple danced , the tango began to be played by a live orchestra .']\n",
    "# sentences_long_unambig = ['Even though the band which played funk music left , the party went on for at least another two hours.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Did not use initialization regex that was passed: .*weight_ih.*\n",
      "Did not use initialization regex that was passed: .*bias_hh.*\n",
      "Did not use initialization regex that was passed: .*weight_hh.*\n",
      "Did not use initialization regex that was passed: .*bias_ih.*\n"
     ]
    }
   ],
   "source": [
    "#lm, lm_vocabulary = load_lm()\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/biaffine-dependency-parser-ptb-2018.08.23.tar.gz\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_dependencies(sent, parser = 'allennlp'):\n",
    "    if parser == 'spacy':\n",
    "        return [token.dep_ for token in nlp(sent)], [str(t) for t in nlp(sent)]\n",
    "    else:\n",
    "        predictions = predictor.predict(sentence =  sent)\n",
    "        return predictions['predicted_dependencies'], predictions['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sentences[0]\n",
    "noun = 'party'\n",
    "sent_portion_1 = ' '.join(sent.split()[:sent.split().index(noun)+1])\n",
    "sent_portion_2 = ' '.join(sent.split()[:sent.split().index(noun)+2])\n",
    "\n",
    "sentences_1 =get_sentences(sent_portion_1, lm, lm_vocabulary)\n",
    "sentences_2 = get_sentences(sent_portion_2, lm, lm_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nsubj', 'root')\n",
      "('nsubj', 'root')\n",
      "Even though the band left the party, they eventually moved to New York City at least twice. ('dobj', 'punct')\n",
      "Even though the band left the party in May, and they were still working on new recordings at a ('dobj', 'prep')\n",
      "Even though the band left the party, his support was still strong and continued in large part because he ('dobj', 'punct')\n",
      "Even though the band left the party for good, and they didn't leave it because of its record ('dobj', 'prep')\n",
      "Even though the band left the party with a new member, his music will undoubtedly be different.\n",
      " ('dobj', 'prep')\n",
      "Even though the band left the party and its new leader, Raul Vazilovic is back ('dobj', 'cc')\n",
      "Even though the band left the party, it was clear there were a lot of other members who had ('dobj', 'punct')\n",
      "Even though the band left the party, their fans will be able to continue with this new album. ('dobj', 'punct')\n",
      "Even though the band left the party with only eight members, many felt betrayed by their own experience in ('dobj', 'prep')\n",
      "Even though the band left the party early in their own career, he was still active on a number ('dobj', 'advmod')\n",
      "Even though the band left the party last year, it had more than two dozen members. ('dobj', 'amod')\n",
      "Even though the band left the party went on to win a major state title, and had more than ('nsubj', 'root')\n",
      "Even though the band left the party went in to fight at a rally against Obama's executive order, ('nsubj', 'root')\n",
      "Even though the band left the party went after President Obama's administration, it has been widely said that ('nsubj', 'ccomp')\n",
      "Even though the band left the party went on and continued into their own career, many people didn't ('nsubj', 'ccomp')\n",
      "Even though the band left the party went down to a 2.3-percent tie with its record ('nsubj', 'root')\n",
      "Even though the band left the party went into an early state of decline, many members were hoping it ('nsubj', 'ccomp')\n",
      "Even though the band left the party went home to their new apartment in Washington, New York where they ('nsubj', 'root')\n",
      "Even though the band left the party went on a hiatus and didn't perform in time to take off ('nsubj', 'root')\n",
      "Even though the band left the party went ahead with their tour, they did not take any of that ('nsubj', 'ccomp')\n",
      "Even though the band left the party went to a rally and was given $3,500 in cash ('nsubj', 'ccomp')\n",
      "Even though the band left the party went on to do an awful lot more with that, they never ('nsubj', 'ccomp')\n"
     ]
    }
   ],
   "source": [
    "def np_z(sent, noun, verb, parser = 'allennlp', add_coma = False, after_verb = False):\n",
    "    if add_coma: sent = sent.replace(verb, verb + ',')\n",
    "    parse, tokens = get_dependencies(sent, parser = parser)\n",
    "    label_np = parse[tokens.index(noun)]\n",
    "    if not after_verb:\n",
    "        label_verb = parse[tokens.index(noun) + 1 ]\n",
    "    if label_np = \n",
    "    return label_np, label_verb\n",
    "\n",
    "verb = 'went'\n",
    "noun = 'party'\n",
    "print(np_z(sentences[0], noun, verb))\n",
    "print(np_z(sentences_unambig[0], noun, verb))\n",
    "\n",
    "for s in sentences_1:\n",
    "    print(s, np_z(s, noun, verb))\n",
    "for s in sentences_2:\n",
    "    print(s, np_z(s, noun, verb, add_coma = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(sent_portion, lm, lm_vocabulary):\n",
    "    sent_portion = lm_vocabulary.encode(sent_portion)\n",
    "    sent_portion = torch.LongTensor(sent_portion).unsqueeze(0)\n",
    "    generated = generate(sent_portion, lm, lm_vocabulary, do_sample = True, \n",
    "                   repetition_penalty = 2, num_return_sequences =  20, temperature = 1,\n",
    "                unknown_penalty = 1000000, top_k = 20, num_beams = 1, max_length = 20)  # generate sequence\n",
    "    sent_generated = []\n",
    "    for i in range(len(generated)):\n",
    "        s = lm_vocabulary.decode(generated[i])\n",
    "        s = list(nlp(s).sents)[0].text\n",
    "        if s not in sent_generated:\n",
    "            sent_generated.append(s)\n",
    "    sent_generated = sent_generated[:11]\n",
    "    return sent_generated\n",
    "\n",
    "# sentences_per_item = []\n",
    "# for sent, split in zip(rrc_sentences, rrc_disambiguating):\n",
    "#     sent = sent.replace('.', ' .')\n",
    "#     sent = sent.split()\n",
    "#     sent_portion_1 = ' '.join(sent[:sent.index(split)])\n",
    "#     sent_portion_2 = ' '.join(sent[:sent.index(split)+1])\n",
    "#     if not lm_vocabulary.encode('<unk>')[0] in lm_vocabulary.encode(sent_portion_2):\n",
    "#         sent1 = get_sentences(sent_portion_1, lm, lm_vocabulary)\n",
    "#         sent2 = get_sentences(sent_portion_2, lm, lm_vocabulary)\n",
    "#         print(sent1, sent2)\n",
    "#         sentences_per_item.append((sent1, sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det volunteers NOUN []\n",
      "sleepy amod volunteers NOUN []\n",
      "volunteers nsubj were AUX [The, sleepy, given]\n",
      "given acl volunteers NOUN [soup]\n",
      "the det soup NOUN []\n",
      "hot amod soup NOUN []\n",
      "soup dobj given VERB [the, hot]\n",
      "were auxpass prepared ADJ [volunteers]\n",
      "prepared ROOT prepared ADJ [were, pour, .]\n",
      "to aux pour VERB []\n",
      "pour xcomp prepared ADJ [to, food]\n",
      "food dobj pour VERB []\n",
      ". punct prepared ADJ []\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The sleepy volunteers given the hot soup were prepared to pour food .\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "pkl.dump(sentences_per_item, open('../data/generated.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
