{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_utils import *\n",
    "from generation_utils import *\n",
    "import pandas as  pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Did not use initialization regex that was passed: .*bias_ih.*\n",
      "Did not use initialization regex that was passed: .*weight_hh.*\n",
      "Did not use initialization regex that was passed: .*bias_hh.*\n",
      "Did not use initialization regex that was passed: .*weight_ih.*\n"
     ]
    }
   ],
   "source": [
    "lm, lm_vocabulary = load_lm()\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/biaffine-dependency-parser-ptb-2018.08.23.tar.gz\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_dependencies(sent, parser = 'spacy'):\n",
    "    if parser == 'spacy':\n",
    "        return [token.dep_ for token in nlp(sent)]\n",
    "    else:\n",
    "        return predictor.predict(sentence =  sent)['predicted_dependencies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrc_data = pd.read_csv('../data/sentences_RRC_2.csv')[:64]\n",
    "parser = 'spacy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrc_before_disambiguation(sent,idx_verbRC, parser = 'spacy'):\n",
    "    dep = get_dependencies(sent, parser = parser)\n",
    "    return dep[idx_verbRC].lower() == 'root', dep\n",
    "        \n",
    "\n",
    "def rrc_after_disambiguation(sent, rrc, verb):\n",
    "    s_no_rrc = sent.replace(' ' + rrc, '')\n",
    "    print(sent, s_no_rrc, verb)\n",
    "    try:\n",
    "        idx_verb = s_no_rrc.split().index(verb)\n",
    "    except:\n",
    "        print(s_no_rrc, verb)\n",
    "    dep = get_dependencies(s_no_rrc, parser = parser)\n",
    "    ok = dep[idx_verb].lower() == 'root' or(verb in ['was', 'were'] and dep[idx_verb] == 'auxpass'and dep[idx_verb + 1].lower() == 'root')\n",
    "    return ok, dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experienced waitress cooked the grilled chicken .\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "replace() argument 1 must be str, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3156b13b79a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msent_portion_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlm_vocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlm_vocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_portion_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0msent1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_portion_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrrc_verb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_vocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0msent2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_portion_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrrc_verb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_vocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_disambiguation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msentences_per_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-3156b13b79a5>\u001b[0m in \u001b[0;36mget_sentences\u001b[0;34m(sent_portion, rrc, verb, rrc_verb, lm, lm_vocabulary, after_disambiguation)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_generated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_portion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msent_portion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0msent_generated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msent_generated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_generated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: replace() argument 1 must be str, not Tensor"
     ]
    }
   ],
   "source": [
    "def get_sentences(prompt, rrc, verb, rrc_verb, lm, lm_vocabulary, after_disambiguation = False):\n",
    "    sent_portion = lm_vocabulary.encode(prompt)\n",
    "    sent_portion = torch.LongTensor(sent_portion).unsqueeze(0)\n",
    "    generated = generate(sent_portion, lm, lm_vocabulary, do_sample = True, \n",
    "                   repetition_penalty = 5, num_return_sequences =  10, temperature = 1,\n",
    "                unknown_penalty = 100, top_k = 100, num_beams = 5, max_length = 20)  # generate sequence\n",
    "    sent_generated = []\n",
    "    for i in range(len(generated)):\n",
    "        s = lm_vocabulary.decode(generated[i])\n",
    "        s = list(nlp(s).sents)[0].text\n",
    "        if s not in sent_generated:\n",
    "            if s.replace(prompt, '') != prompt:\n",
    "                sent_generated.append(s)\n",
    "    sent_generated = sent_generated[:11]\n",
    "    print(sent_generated)\n",
    "    return sent_generated\n",
    "\n",
    "def check_sentences(sent_generated, sent_portion, rrc, verb, rrc_verb, after_disambiguation = False):\n",
    "    for s in sent_generated:\n",
    "        if not after_disambiguation:\n",
    "            ok, dep = rrc_before_disambiguation(s, s.split().index(rrc_verb))\n",
    "        else:\n",
    "            ok, dep = rrc_after_disambiguation(s, rrc, verb) \n",
    "#             print(ok, s)\n",
    "sentences_per_item = []\n",
    "for _, row in rrc_data.iterrows():\n",
    "    sent = row.RRC_sentence\n",
    "    verb = row.verb\n",
    "    rrc = row.RRC\n",
    "    rrc_verb = row.RRC_verb\n",
    "    sent = sent.replace('.', ' .')\n",
    "    sent = sent.split()\n",
    "    sent_portion_1 = ' '.join(sent[:sent.index(verb)])\n",
    "    sent_portion_2 = ' '.join(sent[:sent.index(verb)+1])\n",
    "    if not lm_vocabulary.encode('<unk>')[0] in lm_vocabulary.encode(sent_portion_2):\n",
    "        sent1 = get_sentences(sent_portion_1, rrc, verb, rrc_verb, lm, lm_vocabulary)\n",
    "        sent2 = get_sentences(sent_portion_2, rrc, verb, rrc_verb, lm, lm_vocabulary, after_disambiguation = True)\n",
    "        sentences_per_item.append((sent1, sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The young kid given the hot soup got ready to go to school . \")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "pkl.dump(sentences_per_item, open('../data/generated.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/biaffine-dependency-parser-ptb-2018.08.23.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
